
TO RUN SPARK ON KUBERNETES IN CLUSTER MODE WE CAN USE THE FOLLOWING COMMAND: 
THE SERVICE ACCOUNT HAS A CLUSTER ADMIN ROLE BUT CAN BE CONFIGURED TO A MORE RESTRICTED ROLE SINCE SPARK DOESN'T NEED THAT MUCH FREEDOM TO CREATE AND MONITORIZE THE EXECUTOR PODS.

spark-submit \
  --master k8s://https://192.168.1.150:6443 \
  --deploy-mode cluster \
  --conf spark.kubernetes.container.image=bitnami/spark:4.0.0 \
  --conf spark.kubernetes.authenticate.driver.serviceAccountName=my-pyspark-notebook \
  --conf spark.executor.instances=2 \
  local:///opt/bitnami/spark/examples/src/main/python/sql/basic.py

AND THAT'S A REGULAR OUTPUT THAT STATES EVERYTHING WENT SMOOTHLY:

WARNING: Using incubator modules: jdk.incubator.vector
25/06/26 10:50:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
25/06/26 10:50:23 INFO SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file
25/06/26 10:50:27 INFO KerberosConfDriverFeatureStep: You have not specified a krb5.conf file locally or via a ConfigMap. Make sure that you have the krb5.conf locally on the driver image.
25/06/26 10:50:30 INFO LoggingPodStatusWatcherImpl: State changed, new state:
         pod name: basic-py-894d2597abdc3051-driver
         namespace: default
         labels: spark-app-name -> basic-py, spark-app-selector -> spark-747d6190b0a042c6bb6b1e5496870a8a, spark-role -> driver, spark-version -> 4.0.0
         pod uid: b999dc80-fc8a-4e4a-b583-9f9d7bb26cb8
         creation time: 2025-06-26T10:50:29Z
         service account name: my-pyspark-notebook
         volumes: spark-local-dir-1, spark-conf-volume-driver, kube-api-access-dqt4v
         node name: ubuntu2
         start time: 2025-06-26T10:50:29Z
         phase: Pending
         container status:
                 container name: spark-kubernetes-driver
                 container image: bitnami/spark:4.0.0
                 container state: waiting
                 pending reason: ContainerCreating
25/06/26 10:50:30 INFO LoggingPodStatusWatcherImpl: State changed, new state:
         pod name: basic-py-894d2597abdc3051-driver
         namespace: default
         labels: spark-app-name -> basic-py, spark-app-selector -> spark-747d6190b0a042c6bb6b1e5496870a8a, spark-role -> driver, spark-version -> 4.0.0
         pod uid: b999dc80-fc8a-4e4a-b583-9f9d7bb26cb8
         creation time: 2025-06-26T10:50:29Z
         service account name: my-pyspark-notebook
         volumes: spark-local-dir-1, spark-conf-volume-driver, kube-api-access-dqt4v
         node name: ubuntu2
         start time: 2025-06-26T10:50:29Z
         phase: Pending
         container status:
                 container name: spark-kubernetes-driver
                 container image: bitnami/spark:4.0.0
                 container state: waiting
                 pending reason: ContainerCreating
25/06/26 10:50:30 INFO LoggingPodStatusWatcherImpl: Waiting for application basic.py} with application ID spark-747d6190b0a042c6bb6b1e5496870a8a and submission ID default:basic-py-894d2597abdc3051-driver to finish...
25/06/26 10:50:31 INFO LoggingPodStatusWatcherImpl: State changed, new state:
         pod name: basic-py-894d2597abdc3051-driver
         namespace: default
         labels: spark-app-name -> basic-py, spark-app-selector -> spark-747d6190b0a042c6bb6b1e5496870a8a, spark-role -> driver, spark-version -> 4.0.0
         pod uid: b999dc80-fc8a-4e4a-b583-9f9d7bb26cb8
         creation time: 2025-06-26T10:50:29Z
         service account name: my-pyspark-notebook
         volumes: spark-local-dir-1, spark-conf-volume-driver, kube-api-access-dqt4v
         node name: ubuntu2
         start time: 2025-06-26T10:50:29Z
         phase: Pending
         container status:
                 container name: spark-kubernetes-driver
                 container image: bitnami/spark:4.0.0
                 container state: waiting
                 pending reason: ContainerCreating
25/06/26 10:50:31 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Pending)
25/06/26 10:50:32 INFO LoggingPodStatusWatcherImpl: State changed, new state:
         pod name: basic-py-894d2597abdc3051-driver
         namespace: default
         labels: spark-app-name -> basic-py, spark-app-selector -> spark-747d6190b0a042c6bb6b1e5496870a8a, spark-role -> driver, spark-version -> 4.0.0
         pod uid: b999dc80-fc8a-4e4a-b583-9f9d7bb26cb8
         creation time: 2025-06-26T10:50:29Z
         service account name: my-pyspark-notebook
         volumes: spark-local-dir-1, spark-conf-volume-driver, kube-api-access-dqt4v
         node name: ubuntu2
         start time: 2025-06-26T10:50:29Z
         phase: Running
         container status:
                 container name: spark-kubernetes-driver
                 container image: docker.io/bitnami/spark:4.0.0
                 container state: running
                 container started at: 2025-06-26T10:50:31Z
25/06/26 10:50:32 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:50:33 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:50:34 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:50:35 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:50:36 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:50:37 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:50:38 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:50:39 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:50:40 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:50:41 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:50:42 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:50:43 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:50:44 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:50:45 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:50:46 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:50:47 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:50:48 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:50:49 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:50:50 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:50:51 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:50:52 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:50:53 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:50:54 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:50:55 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:50:56 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:50:57 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:50:58 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:50:59 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:51:00 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:51:02 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:51:03 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:51:04 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:51:05 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:51:06 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:51:07 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:51:08 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:51:09 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:51:10 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:51:11 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:51:12 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:51:13 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:51:14 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:51:15 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:51:16 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:51:17 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:51:18 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:51:19 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:51:20 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:51:20 INFO LoggingPodStatusWatcherImpl: State changed, new state:
         pod name: basic-py-894d2597abdc3051-driver
         namespace: default
         labels: spark-app-name -> basic-py, spark-app-selector -> spark-747d6190b0a042c6bb6b1e5496870a8a, spark-role -> driver, spark-version -> 4.0.0
         pod uid: b999dc80-fc8a-4e4a-b583-9f9d7bb26cb8
         creation time: 2025-06-26T10:50:29Z
         service account name: my-pyspark-notebook
         volumes: spark-local-dir-1, spark-conf-volume-driver, kube-api-access-dqt4v
         node name: ubuntu2
         start time: 2025-06-26T10:50:29Z
         phase: Running
         container status:
                 container name: spark-kubernetes-driver
                 container image: docker.io/bitnami/spark:4.0.0
                 container state: terminated
                 container started at: 2025-06-26T10:50:31Z
                 container finished at: 2025-06-26T10:51:19Z
                 exit code: 0
                 termination reason: Completed
25/06/26 10:51:21 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Running)
25/06/26 10:51:21 INFO LoggingPodStatusWatcherImpl: State changed, new state:
         pod name: basic-py-894d2597abdc3051-driver
         namespace: default
         labels: spark-app-name -> basic-py, spark-app-selector -> spark-747d6190b0a042c6bb6b1e5496870a8a, spark-role -> driver, spark-version -> 4.0.0
         pod uid: b999dc80-fc8a-4e4a-b583-9f9d7bb26cb8
         creation time: 2025-06-26T10:50:29Z
         service account name: my-pyspark-notebook
         volumes: spark-local-dir-1, spark-conf-volume-driver, kube-api-access-dqt4v
         node name: ubuntu2
         start time: 2025-06-26T10:50:29Z
         phase: Running
         container status:
                 container name: spark-kubernetes-driver
                 container image: docker.io/bitnami/spark:4.0.0
                 container state: terminated
                 container started at: 2025-06-26T10:50:31Z
                 container finished at: 2025-06-26T10:51:19Z
                 exit code: 0
                 termination reason: Completed
25/06/26 10:51:21 INFO LoggingPodStatusWatcherImpl: State changed, new state:
         pod name: basic-py-894d2597abdc3051-driver
         namespace: default
         labels: spark-app-name -> basic-py, spark-app-selector -> spark-747d6190b0a042c6bb6b1e5496870a8a, spark-role -> driver, spark-version -> 4.0.0
         pod uid: b999dc80-fc8a-4e4a-b583-9f9d7bb26cb8
         creation time: 2025-06-26T10:50:29Z
         service account name: my-pyspark-notebook
         volumes: spark-local-dir-1, spark-conf-volume-driver, kube-api-access-dqt4v
         node name: ubuntu2
         start time: 2025-06-26T10:50:29Z
         phase: Succeeded
         container status:
                 container name: spark-kubernetes-driver
                 container image: docker.io/bitnami/spark:4.0.0
                 container state: terminated
                 container started at: 2025-06-26T10:50:31Z
                 container finished at: 2025-06-26T10:51:19Z
                 exit code: 0
                 termination reason: Completed
25/06/26 10:51:21 INFO LoggingPodStatusWatcherImpl: Application status for spark-747d6190b0a042c6bb6b1e5496870a8a (phase: Succeeded)
25/06/26 10:51:21 INFO LoggingPodStatusWatcherImpl: Container final statuses:


         container name: spark-kubernetes-driver
         container image: docker.io/bitnami/spark:4.0.0
         container state: terminated
         container started at: 2025-06-26T10:50:31Z
         container finished at: 2025-06-26T10:51:19Z
         exit code: 0
         termination reason: Completed
25/06/26 10:51:21 INFO LoggingPodStatusWatcherImpl: Application basic.py with application ID spark-747d6190b0a042c6bb6b1e5496870a8a and submission ID default:basic-py-894d2597abdc3051-driver finished
25/06/26 10:51:21 INFO ShutdownHookManager: Shutdown hook called
25/06/26 10:51:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-4cf2bd7c-21d6-4212-91d9-54c0a5d9061c


IF WE WANT TO RUN IT IN CLIENT MODE, THERE ARE VARIOS WAYS. SOMETIMES PEOPLE RUN THE DRIVER OUTSIDE OF THE KUBERNETES CLUSTER. I USED THE COMPUTER THAT CONTAINED THE CONTROL PLANE IN A VIRTUAL MACHINE AS THE EXTERNAL CLIENT AS WELL, IT'S NOTICEABLE ON THE COMMAND BELOW.

IN THIS CASE I USE PYSPARK RUNNING IN THE CONSOLE AND FOR THE EXECUTORS I USE THE BITNAMI IMAGE FROM DOCKER HUB, IT IS A VERY GOOD OPTION SINCE IT IS COMPATIBLE WITH AMR64 AND AMD64 ARQUITECTURES AT THE SAME TIME, IT'S VERY WELL MAINTAINED, AND WE CAN FIND NO MATTER WHAT VERSION OF SPARK WE NEED EVEN FOR THE MOST RECENT VERSION SUCH AS 4.0.0 WHICH IS CONSIDERABLY NEW ON THE TIME I'M WRITING THAT:

vboxuser@controlplane:~/spark-4.0.0-bin-hadoop3/bin$ pyspark \
  --master k8s://https://192.168.1.150:6443 \
  --conf spark.submit.deployMode=client \
  --conf spark.driver.host=192.168.1.150 \
  --conf spark.driver.bindAddress=0.0.0.0 \
  --conf spark.executor.instances=2 \
  --conf spark.kubernetes.container.image=bitnami/spark:4.0.0 \
  --conf spark.kubernetes.executor.deleteOnTermination=true

AND HERE IS THE OUTPUT:

Python 3.12.3 (main, Jun 18 2025, 17:59:45) [GCC 13.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
WARNING: Using incubator modules: jdk.incubator.vector
25/06/26 10:57:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 4.0.0
      /_/

Using Python version 3.12.3 (main, Jun 18 2025 17:59:45)
Spark context Web UI available at http://192.168.1.150:4040
Spark context available as 'sc' (master = k8s://https://192.168.1.150:6443, app id = spark-5ecc5ad6b3424f6188c11358fad5fba4).
SparkSession available as 'spark'.
>>> data = [("Alice", 34), ("Bob", 45), ("Charlie", 29)]
>>> columns = ["name", "age"]
>>> df = spark.createDataFrame(data, columns)
>>> df.show()
+-------+---+
|   name|age|
+-------+---+
|  Alice| 34|
|    Bob| 45|
|Charlie| 29|
+-------+---+


LET'S SEE NOW HOW WE CAN USE A JUPYTER LAB NOTEBOOK TO RUN OUR PYSPARK ON HAVING WITH AN EXTERNAL CLIENT (I HAVE TO FIRSTLY CONNECT TO THE CONTROL PLANE THROUGH SSH), THIS PROCESS IS USEFUL BECAUSE IT LET'S YOU SEE THE NECESSARY DEPENDENCIES YOU NEED TO INSTALL IN YOUR ENVIRONMENT BEFORE PYSPARK CAN BE USED AND WILL HELP US UNDERSTAND WHAT WE SHOULD INCLUDE IN A DOCKER IMAGE TO BE USED ON THE KUBERNETES POD WHEN WE RUN THE DRIVER INSIDE THE CLUSTER AS A POD: 

PS C:\WINDOWS\system32> ssh vboxuser@192.168.1.150
vboxuser@192.168.1.150's password:
Welcome to Ubuntu 24.04.2 LTS (GNU/Linux 6.8.0-62-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/pro

 System information as of Thu Jun 26 10:29:39 AM UTC 2025

  System load:  2.7                Processes:               224
  Usage of /:   37.8% of 97.87GB   Users logged in:         1
  Memory usage: 15%                IPv4 address for enp0s3: 192.168.1.150
  Swap usage:   0%

 * Strictly confined Kubernetes makes edge and IoT secure. Learn how MicroK8s
   just raised the bar for easy, resilient and secure K8s cluster deployment.

   https://ubuntu.com/engage/secure-kubernetes-at-the-edge

Expanded Security Maintenance for Applications is not enabled.

58 updates can be applied immediately.
2 of these updates are standard security updates.
To see these additional updates run: apt list --upgradable

Enable ESM Apps to receive additional future security updates.
See https://ubuntu.com/esm or run: sudo pro status


Last login: Thu Jun 26 09:58:46 2025 from 192.168.1.131
vboxuser@controlplane:~$ sudo apt install python3 python3-pip -y
[sudo] password for vboxuser:
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
python3 is already the newest version (3.12.3-0ubuntu2).
python3 set to manually installed.
The following packages were automatically installed and are no longer required:
  bridge-utils dns-root-data dnsmasq-base pigz ubuntu-fan
Use 'sudo apt autoremove' to remove them.
The following additional packages will be installed:
  javascript-common libexpat1-dev libjs-jquery libjs-sphinxdoc libjs-underscore libpython3-dev libpython3.12-dev python3-dev python3-wheel python3.12-dev zlib1g-dev
Suggested packages:
  apache2 | lighttpd | httpd
The following NEW packages will be installed:
  javascript-common libexpat1-dev libjs-jquery libjs-sphinxdoc libjs-underscore libpython3-dev libpython3.12-dev python3-dev python3-pip python3-wheel python3.12-dev zlib1g-dev
0 upgraded, 12 newly installed, 0 to remove and 59 not upgraded.
Need to get 9,220 kB of archives.
After this operation, 41.2 MB of additional disk space will be used.
Get:1 http://es.archive.ubuntu.com/ubuntu noble/main amd64 javascript-common all 11+nmu1 [5,936 B]
Get:2 http://es.archive.ubuntu.com/ubuntu noble-updates/main amd64 libexpat1-dev amd64 2.6.1-2ubuntu0.3 [140 kB]
Get:3 http://es.archive.ubuntu.com/ubuntu noble/main amd64 libjs-jquery all 3.6.1+dfsg+~3.5.14-1 [328 kB]
Get:4 http://es.archive.ubuntu.com/ubuntu noble/main amd64 libjs-underscore all 1.13.4~dfsg+~1.11.4-3 [118 kB]
Get:5 http://es.archive.ubuntu.com/ubuntu noble/main amd64 libjs-sphinxdoc all 7.2.6-6 [149 kB]
Get:6 http://es.archive.ubuntu.com/ubuntu noble-updates/main amd64 zlib1g-dev amd64 1:1.3.dfsg-3.1ubuntu2.1 [894 kB]
Get:7 http://es.archive.ubuntu.com/ubuntu noble-updates/main amd64 libpython3.12-dev amd64 3.12.3-1ubuntu0.7 [5,680 kB]
Get:8 http://es.archive.ubuntu.com/ubuntu noble-updates/main amd64 libpython3-dev amd64 3.12.3-0ubuntu2 [10.3 kB]
Get:9 http://es.archive.ubuntu.com/ubuntu noble-updates/main amd64 python3.12-dev amd64 3.12.3-1ubuntu0.7 [498 kB]
Get:10 http://es.archive.ubuntu.com/ubuntu noble-updates/main amd64 python3-dev amd64 3.12.3-0ubuntu2 [26.7 kB]
Get:11 http://es.archive.ubuntu.com/ubuntu noble/universe amd64 python3-wheel all 0.42.0-2 [53.1 kB]
Get:12 http://es.archive.ubuntu.com/ubuntu noble-updates/universe amd64 python3-pip all 24.0+dfsg-1ubuntu1.1 [1,317 kB]
Fetched 9,220 kB in 1s (12.2 MB/s)
Selecting previously unselected package javascript-common.
(Reading database ... 142542 files and directories currently installed.)
Preparing to unpack .../00-javascript-common_11+nmu1_all.deb ...
Unpacking javascript-common (11+nmu1) ...
Selecting previously unselected package libexpat1-dev:amd64.
Preparing to unpack .../01-libexpat1-dev_2.6.1-2ubuntu0.3_amd64.deb ...
Unpacking libexpat1-dev:amd64 (2.6.1-2ubuntu0.3) ...
Selecting previously unselected package libjs-jquery.
Preparing to unpack .../02-libjs-jquery_3.6.1+dfsg+~3.5.14-1_all.deb ...
Unpacking libjs-jquery (3.6.1+dfsg+~3.5.14-1) ...
Selecting previously unselected package libjs-underscore.
Preparing to unpack .../03-libjs-underscore_1.13.4~dfsg+~1.11.4-3_all.deb ...
Unpacking libjs-underscore (1.13.4~dfsg+~1.11.4-3) ...
Selecting previously unselected package libjs-sphinxdoc.
Preparing to unpack .../04-libjs-sphinxdoc_7.2.6-6_all.deb ...
Unpacking libjs-sphinxdoc (7.2.6-6) ...
Selecting previously unselected package zlib1g-dev:amd64.
Preparing to unpack .../05-zlib1g-dev_1%3a1.3.dfsg-3.1ubuntu2.1_amd64.deb ...
Unpacking zlib1g-dev:amd64 (1:1.3.dfsg-3.1ubuntu2.1) ...
Selecting previously unselected package libpython3.12-dev:amd64.
Preparing to unpack .../06-libpython3.12-dev_3.12.3-1ubuntu0.7_amd64.deb ...
Unpacking libpython3.12-dev:amd64 (3.12.3-1ubuntu0.7) ...
Selecting previously unselected package libpython3-dev:amd64.
Preparing to unpack .../07-libpython3-dev_3.12.3-0ubuntu2_amd64.deb ...
Unpacking libpython3-dev:amd64 (3.12.3-0ubuntu2) ...
Selecting previously unselected package python3.12-dev.
Preparing to unpack .../08-python3.12-dev_3.12.3-1ubuntu0.7_amd64.deb ...
Unpacking python3.12-dev (3.12.3-1ubuntu0.7) ...
Selecting previously unselected package python3-dev.
Preparing to unpack .../09-python3-dev_3.12.3-0ubuntu2_amd64.deb ...
Unpacking python3-dev (3.12.3-0ubuntu2) ...
Selecting previously unselected package python3-wheel.
Preparing to unpack .../10-python3-wheel_0.42.0-2_all.deb ...
Unpacking python3-wheel (0.42.0-2) ...
Selecting previously unselected package python3-pip.
Preparing to unpack .../11-python3-pip_24.0+dfsg-1ubuntu1.1_all.deb ...
Unpacking python3-pip (24.0+dfsg-1ubuntu1.1) ...
Setting up javascript-common (11+nmu1) ...
Setting up python3-wheel (0.42.0-2) ...
Setting up libexpat1-dev:amd64 (2.6.1-2ubuntu0.3) ...
Setting up python3-pip (24.0+dfsg-1ubuntu1.1) ...
Setting up zlib1g-dev:amd64 (1:1.3.dfsg-3.1ubuntu2.1) ...
Setting up libjs-jquery (3.6.1+dfsg+~3.5.14-1) ...
Setting up libjs-underscore (1.13.4~dfsg+~1.11.4-3) ...
Setting up libpython3.12-dev:amd64 (3.12.3-1ubuntu0.7) ...
Setting up python3.12-dev (3.12.3-1ubuntu0.7) ...
Setting up libjs-sphinxdoc (7.2.6-6) ...
Setting up libpython3-dev:amd64 (3.12.3-0ubuntu2) ...
Setting up python3-dev (3.12.3-0ubuntu2) ...
Processing triggers for man-db (2.12.0-4build2) ...
Scanning processes...
Scanning linux images...

Running kernel seems to be up-to-date.

No services need to be restarted.

No containers need to be restarted.

No user sessions are running outdated binaries.

No VM guests are running outdated hypervisor (qemu) binaries on this host.
vboxuser@controlplane:~$ pip3 install jupyterlab
error: externally-managed-environment

× This environment is externally managed
╰─> To install Python packages system-wide, try apt install
    python3-xyz, where xyz is the package you are trying to
    install.

    If you wish to install a non-Debian-packaged Python package,
    create a virtual environment using python3 -m venv path/to/venv.
    Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make
    sure you have python3-full installed.

    If you wish to install a non-Debian packaged Python application,
    it may be easiest to use pipx install xyz, which will manage a
    virtual environment for you. Make sure you have pipx installed.

    See /usr/share/doc/python3.12/README.venv for more information.

note: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.
hint: See PEP 668 for the detailed specification.
vboxuser@controlplane:~$ python3 -m venv ~/myenv
The virtual environment was not created successfully because ensurepip is not
available.  On Debian/Ubuntu systems, you need to install the python3-venv
package using the following command.

    apt install python3.12-venv

You may need to use sudo with that command.  After installing the python3-venv
package, recreate your virtual environment.

Failing command: /home/vboxuser/myenv/bin/python3

vboxuser@controlplane:~$ sudo apt install python3.12-venv
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
The following packages were automatically installed and are no longer required:
  bridge-utils dns-root-data dnsmasq-base pigz ubuntu-fan
Use 'sudo apt autoremove' to remove them.
The following additional packages will be installed:
  python3-pip-whl python3-setuptools-whl
The following NEW packages will be installed:
  python3-pip-whl python3-setuptools-whl python3.12-venv
0 upgraded, 3 newly installed, 0 to remove and 59 not upgraded.
Need to get 2,425 kB of archives.
After this operation, 2,771 kB of additional disk space will be used.
Do you want to continue? [Y/n] Y
Get:1 http://es.archive.ubuntu.com/ubuntu noble-updates/universe amd64 python3-pip-whl all 24.0+dfsg-1ubuntu1.1 [1,703 kB]
Get:2 http://es.archive.ubuntu.com/ubuntu noble-updates/universe amd64 python3-setuptools-whl all 68.1.2-2ubuntu1.2 [716 kB]
Get:3 http://es.archive.ubuntu.com/ubuntu noble-updates/universe amd64 python3.12-venv amd64 3.12.3-1ubuntu0.7 [5,680 B]
Fetched 2,425 kB in 0s (5,071 kB/s)
Selecting previously unselected package python3-pip-whl.
(Reading database ... 143653 files and directories currently installed.)
Preparing to unpack .../python3-pip-whl_24.0+dfsg-1ubuntu1.1_all.deb ...
Unpacking python3-pip-whl (24.0+dfsg-1ubuntu1.1) ...
Selecting previously unselected package python3-setuptools-whl.
Preparing to unpack .../python3-setuptools-whl_68.1.2-2ubuntu1.2_all.deb ...
Unpacking python3-setuptools-whl (68.1.2-2ubuntu1.2) ...
Selecting previously unselected package python3.12-venv.
Preparing to unpack .../python3.12-venv_3.12.3-1ubuntu0.7_amd64.deb ...
Unpacking python3.12-venv (3.12.3-1ubuntu0.7) ...
Setting up python3-setuptools-whl (68.1.2-2ubuntu1.2) ...
Setting up python3-pip-whl (24.0+dfsg-1ubuntu1.1) ...
Setting up python3.12-venv (3.12.3-1ubuntu0.7) ...
Scanning processes...
Scanning linux images...

Running kernel seems to be up-to-date.

No services need to be restarted.

No containers need to be restarted.

No user sessions are running outdated binaries.

No VM guests are running outdated hypervisor (qemu) binaries on this host.
vboxuser@controlplane:~$ python3 -m venv ~/myenv
vboxuser@controlplane:~$ source ~/myenv/bin/activate
(myenv) vboxuser@controlplane:~$ pip install --upgrade pip
Requirement already satisfied: pip in ./myenv/lib/python3.12/site-packages (24.0)
Collecting pip
  Downloading pip-25.1.1-py3-none-any.whl.metadata (3.6 kB)
Downloading pip-25.1.1-py3-none-any.whl (1.8 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/1.8 MB 13.4 MB/s eta 0:00:00
Installing collected packages: pip
  Attempting uninstall: pip
    Found existing installation: pip 24.0
    Uninstalling pip-24.0:
      Successfully uninstalled pip-24.0
Successfully installed pip-25.1.1
(myenv) vboxuser@controlplane:~$ pip install jupyterlab
Collecting jupyterlab
  Downloading jupyterlab-4.4.3-py3-none-any.whl.metadata (16 kB)
Collecting async-lru>=1.0.0 (from jupyterlab)
  Downloading async_lru-2.0.5-py3-none-any.whl.metadata (4.5 kB)
Collecting httpx>=0.25.0 (from jupyterlab)
  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)
Collecting ipykernel>=6.5.0 (from jupyterlab)
  Downloading ipykernel-6.29.5-py3-none-any.whl.metadata (6.3 kB)
Collecting jinja2>=3.0.3 (from jupyterlab)
  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)
Collecting jupyter-core (from jupyterlab)
  Downloading jupyter_core-5.8.1-py3-none-any.whl.metadata (1.6 kB)
Collecting jupyter-lsp>=2.0.0 (from jupyterlab)
  Downloading jupyter_lsp-2.2.5-py3-none-any.whl.metadata (1.8 kB)
Collecting jupyter-server<3,>=2.4.0 (from jupyterlab)
  Downloading jupyter_server-2.16.0-py3-none-any.whl.metadata (8.5 kB)
Collecting jupyterlab-server<3,>=2.27.1 (from jupyterlab)
  Downloading jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)
Collecting notebook-shim>=0.2 (from jupyterlab)
  Downloading notebook_shim-0.2.4-py3-none-any.whl.metadata (4.0 kB)
Collecting packaging (from jupyterlab)
  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)
Collecting setuptools>=41.1.0 (from jupyterlab)
  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)
Collecting tornado>=6.2.0 (from jupyterlab)
  Downloading tornado-6.5.1-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.8 kB)
Collecting traitlets (from jupyterlab)
  Downloading traitlets-5.14.3-py3-none-any.whl.metadata (10 kB)
Collecting anyio>=3.1.0 (from jupyter-server<3,>=2.4.0->jupyterlab)
  Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)
Collecting argon2-cffi>=21.1 (from jupyter-server<3,>=2.4.0->jupyterlab)
  Downloading argon2_cffi-25.1.0-py3-none-any.whl.metadata (4.1 kB)
Collecting jupyter-client>=7.4.4 (from jupyter-server<3,>=2.4.0->jupyterlab)
  Downloading jupyter_client-8.6.3-py3-none-any.whl.metadata (8.3 kB)
Collecting jupyter-events>=0.11.0 (from jupyter-server<3,>=2.4.0->jupyterlab)
  Downloading jupyter_events-0.12.0-py3-none-any.whl.metadata (5.8 kB)
Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->jupyterlab)
  Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl.metadata (5.6 kB)
Collecting nbconvert>=6.4.4 (from jupyter-server<3,>=2.4.0->jupyterlab)
  Downloading nbconvert-7.16.6-py3-none-any.whl.metadata (8.5 kB)
Collecting nbformat>=5.3.0 (from jupyter-server<3,>=2.4.0->jupyterlab)
  Downloading nbformat-5.10.4-py3-none-any.whl.metadata (3.6 kB)
Collecting overrides>=5.0 (from jupyter-server<3,>=2.4.0->jupyterlab)
  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)
Collecting prometheus-client>=0.9 (from jupyter-server<3,>=2.4.0->jupyterlab)
  Downloading prometheus_client-0.22.1-py3-none-any.whl.metadata (1.9 kB)
Collecting pyzmq>=24 (from jupyter-server<3,>=2.4.0->jupyterlab)
  Downloading pyzmq-27.0.0-cp312-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.0 kB)
Collecting send2trash>=1.8.2 (from jupyter-server<3,>=2.4.0->jupyterlab)
  Downloading Send2Trash-1.8.3-py3-none-any.whl.metadata (4.0 kB)
Collecting terminado>=0.8.3 (from jupyter-server<3,>=2.4.0->jupyterlab)
  Downloading terminado-0.18.1-py3-none-any.whl.metadata (5.8 kB)
Collecting websocket-client>=1.7 (from jupyter-server<3,>=2.4.0->jupyterlab)
  Downloading websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)
Collecting babel>=2.10 (from jupyterlab-server<3,>=2.27.1->jupyterlab)
  Downloading babel-2.17.0-py3-none-any.whl.metadata (2.0 kB)
Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.27.1->jupyterlab)
  Downloading json5-0.12.0-py3-none-any.whl.metadata (36 kB)
Collecting jsonschema>=4.18.0 (from jupyterlab-server<3,>=2.27.1->jupyterlab)
  Downloading jsonschema-4.24.0-py3-none-any.whl.metadata (7.8 kB)
Collecting requests>=2.31 (from jupyterlab-server<3,>=2.27.1->jupyterlab)
  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)
Collecting idna>=2.8 (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->jupyterlab)
  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)
Collecting sniffio>=1.1 (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->jupyterlab)
  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)
Collecting typing_extensions>=4.5 (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->jupyterlab)
  Downloading typing_extensions-4.14.0-py3-none-any.whl.metadata (3.0 kB)
Collecting argon2-cffi-bindings (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab)
  Downloading argon2_cffi_bindings-21.2.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)
Collecting certifi (from httpx>=0.25.0->jupyterlab)
  Downloading certifi-2025.6.15-py3-none-any.whl.metadata (2.4 kB)
Collecting httpcore==1.* (from httpx>=0.25.0->jupyterlab)
  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)
Collecting h11>=0.16 (from httpcore==1.*->httpx>=0.25.0->jupyterlab)
  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)
Collecting comm>=0.1.1 (from ipykernel>=6.5.0->jupyterlab)
  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)
Collecting debugpy>=1.6.5 (from ipykernel>=6.5.0->jupyterlab)
  Downloading debugpy-1.8.14-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)
Collecting ipython>=7.23.1 (from ipykernel>=6.5.0->jupyterlab)
  Downloading ipython-9.3.0-py3-none-any.whl.metadata (4.4 kB)
Collecting matplotlib-inline>=0.1 (from ipykernel>=6.5.0->jupyterlab)
  Downloading matplotlib_inline-0.1.7-py3-none-any.whl.metadata (3.9 kB)
Collecting nest-asyncio (from ipykernel>=6.5.0->jupyterlab)
  Downloading nest_asyncio-1.6.0-py3-none-any.whl.metadata (2.8 kB)
Collecting psutil (from ipykernel>=6.5.0->jupyterlab)
  Downloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)
Collecting decorator (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab)
  Downloading decorator-5.2.1-py3-none-any.whl.metadata (3.9 kB)
Collecting ipython-pygments-lexers (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab)
  Downloading ipython_pygments_lexers-1.1.1-py3-none-any.whl.metadata (1.1 kB)
Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab)
  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)
Collecting pexpect>4.3 (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab)
  Downloading pexpect-4.9.0-py2.py3-none-any.whl.metadata (2.5 kB)
Collecting prompt_toolkit<3.1.0,>=3.0.41 (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab)
  Downloading prompt_toolkit-3.0.51-py3-none-any.whl.metadata (6.4 kB)
Collecting pygments>=2.4.0 (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab)
  Downloading pygments-2.19.2-py3-none-any.whl.metadata (2.5 kB)
Collecting stack_data (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab)
  Downloading stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)
Collecting wcwidth (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab)
  Downloading wcwidth-0.2.13-py2.py3-none-any.whl.metadata (14 kB)
Collecting parso<0.9.0,>=0.8.4 (from jedi>=0.16->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab)
  Downloading parso-0.8.4-py2.py3-none-any.whl.metadata (7.7 kB)
Collecting MarkupSafe>=2.0 (from jinja2>=3.0.3->jupyterlab)
  Downloading MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)
Collecting attrs>=22.2.0 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab)
  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)
Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab)
  Downloading jsonschema_specifications-2025.4.1-py3-none-any.whl.metadata (2.9 kB)
Collecting referencing>=0.28.4 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab)
  Downloading referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)
Collecting rpds-py>=0.7.1 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab)
  Downloading rpds_py-0.25.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)
Collecting python-dateutil>=2.8.2 (from jupyter-client>=7.4.4->jupyter-server<3,>=2.4.0->jupyterlab)
  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)
Collecting platformdirs>=2.5 (from jupyter-core->jupyterlab)
  Downloading platformdirs-4.3.8-py3-none-any.whl.metadata (12 kB)
Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab)
  Downloading python_json_logger-3.3.0-py3-none-any.whl.metadata (4.0 kB)
Collecting pyyaml>=5.3 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab)
  Downloading PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)
Collecting rfc3339-validator (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab)
  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)
Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab)
  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)
Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab)
  Downloading fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)
Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab)
  Downloading isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)
Collecting jsonpointer>1.13 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab)
  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)
Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab)
  Downloading uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)
Collecting webcolors>=24.6.0 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab)
  Downloading webcolors-24.11.1-py3-none-any.whl.metadata (2.2 kB)
Collecting beautifulsoup4 (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab)
  Downloading beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)
Collecting bleach!=5.0.0 (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab)
  Downloading bleach-6.2.0-py3-none-any.whl.metadata (30 kB)
Collecting defusedxml (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab)
  Downloading defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)
Collecting jupyterlab-pygments (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab)
  Downloading jupyterlab_pygments-0.3.0-py3-none-any.whl.metadata (4.4 kB)
Collecting mistune<4,>=2.0.3 (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab)
  Downloading mistune-3.1.3-py3-none-any.whl.metadata (1.8 kB)
Collecting nbclient>=0.5.0 (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab)
  Downloading nbclient-0.10.2-py3-none-any.whl.metadata (8.3 kB)
Collecting pandocfilters>=1.4.1 (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab)
  Downloading pandocfilters-1.5.1-py2.py3-none-any.whl.metadata (9.0 kB)
Collecting webencodings (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab)
  Downloading webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)
Collecting tinycss2<1.5,>=1.1.0 (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab)
  Downloading tinycss2-1.4.0-py3-none-any.whl.metadata (3.0 kB)
Collecting fastjsonschema>=2.15 (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->jupyterlab)
  Downloading fastjsonschema-2.21.1-py3-none-any.whl.metadata (2.2 kB)
Collecting ptyprocess>=0.5 (from pexpect>4.3->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab)
  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl.metadata (1.3 kB)
Collecting six>=1.5 (from python-dateutil>=2.8.2->jupyter-client>=7.4.4->jupyter-server<3,>=2.4.0->jupyterlab)
  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)
Collecting charset_normalizer<4,>=2 (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab)
  Downloading charset_normalizer-3.4.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)
Collecting urllib3<3,>=1.21.1 (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab)
  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)
Collecting cffi>=1.0.1 (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab)
  Downloading cffi-1.17.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)
Collecting pycparser (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab)
  Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)
Collecting soupsieve>1.2 (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab)
  Downloading soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)
Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab)
  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)
Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab)
  Downloading types_python_dateutil-2.9.0.20250516-py3-none-any.whl.metadata (2.1 kB)
Collecting executing>=1.2.0 (from stack_data->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab)
  Downloading executing-2.2.0-py2.py3-none-any.whl.metadata (8.9 kB)
Collecting asttokens>=2.1.0 (from stack_data->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab)
  Downloading asttokens-3.0.0-py3-none-any.whl.metadata (4.7 kB)
Collecting pure-eval (from stack_data->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab)
  Downloading pure_eval-0.2.3-py3-none-any.whl.metadata (6.3 kB)
Downloading jupyterlab-4.4.3-py3-none-any.whl (12.3 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.3/12.3 MB 18.8 MB/s eta 0:00:00
Downloading jupyter_server-2.16.0-py3-none-any.whl (386 kB)
Downloading jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)
Downloading anyio-4.9.0-py3-none-any.whl (100 kB)
Downloading argon2_cffi-25.1.0-py3-none-any.whl (14 kB)
Downloading async_lru-2.0.5-py3-none-any.whl (6.1 kB)
Downloading babel-2.17.0-py3-none-any.whl (10.2 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.2/10.2 MB 22.8 MB/s eta 0:00:00
Downloading httpx-0.28.1-py3-none-any.whl (73 kB)
Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)
Downloading h11-0.16.0-py3-none-any.whl (37 kB)
Downloading idna-3.10-py3-none-any.whl (70 kB)
Downloading ipykernel-6.29.5-py3-none-any.whl (117 kB)
Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)
Downloading debugpy-1.8.14-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.2/4.2 MB 17.7 MB/s eta 0:00:00
Downloading ipython-9.3.0-py3-none-any.whl (605 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 605.3/605.3 kB 5.8 MB/s eta 0:00:00
Downloading prompt_toolkit-3.0.51-py3-none-any.whl (387 kB)
Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 11.0 MB/s eta 0:00:00
Downloading parso-0.8.4-py2.py3-none-any.whl (103 kB)
Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)
Downloading json5-0.12.0-py3-none-any.whl (36 kB)
Downloading jsonschema-4.24.0-py3-none-any.whl (88 kB)
Downloading attrs-25.3.0-py3-none-any.whl (63 kB)
Downloading jsonschema_specifications-2025.4.1-py3-none-any.whl (18 kB)
Downloading jupyter_client-8.6.3-py3-none-any.whl (106 kB)
Downloading jupyter_core-5.8.1-py3-none-any.whl (28 kB)
Downloading jupyter_events-0.12.0-py3-none-any.whl (19 kB)
Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)
Downloading jupyter_lsp-2.2.5-py3-none-any.whl (69 kB)
Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)
Downloading MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)
Downloading matplotlib_inline-0.1.7-py3-none-any.whl (9.9 kB)
Downloading nbconvert-7.16.6-py3-none-any.whl (258 kB)
Downloading mistune-3.1.3-py3-none-any.whl (53 kB)
Downloading bleach-6.2.0-py3-none-any.whl (163 kB)
Downloading tinycss2-1.4.0-py3-none-any.whl (26 kB)
Downloading nbclient-0.10.2-py3-none-any.whl (25 kB)
Downloading nbformat-5.10.4-py3-none-any.whl (78 kB)
Downloading fastjsonschema-2.21.1-py3-none-any.whl (23 kB)
Downloading notebook_shim-0.2.4-py3-none-any.whl (13 kB)
Downloading overrides-7.7.0-py3-none-any.whl (17 kB)
Downloading packaging-25.0-py3-none-any.whl (66 kB)
Downloading pandocfilters-1.5.1-py2.py3-none-any.whl (8.7 kB)
Downloading pexpect-4.9.0-py2.py3-none-any.whl (63 kB)
Downloading platformdirs-4.3.8-py3-none-any.whl (18 kB)
Downloading prometheus_client-0.22.1-py3-none-any.whl (58 kB)
Downloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)
Downloading pygments-2.19.2-py3-none-any.whl (1.2 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 9.9 MB/s eta 0:00:00
Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)
Downloading python_json_logger-3.3.0-py3-none-any.whl (15 kB)
Downloading PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (767 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 767.5/767.5 kB 6.9 MB/s eta 0:00:00
Downloading pyzmq-27.0.0-cp312-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (839 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 839.8/839.8 kB 14.1 MB/s eta 0:00:00
Downloading referencing-0.36.2-py3-none-any.whl (26 kB)
Downloading requests-2.32.4-py3-none-any.whl (64 kB)
Downloading charset_normalizer-3.4.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (148 kB)
Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)
Downloading certifi-2025.6.15-py3-none-any.whl (157 kB)
Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)
Downloading rpds_py-0.25.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (390 kB)
Downloading Send2Trash-1.8.3-py3-none-any.whl (18 kB)
Downloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 6.9 MB/s eta 0:00:00
Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)
Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)
Downloading terminado-0.18.1-py3-none-any.whl (14 kB)
Downloading tornado-6.5.1-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (443 kB)
Downloading traitlets-5.14.3-py3-none-any.whl (85 kB)
Downloading typing_extensions-4.14.0-py3-none-any.whl (43 kB)
Downloading webcolors-24.11.1-py3-none-any.whl (14 kB)
Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)
Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)
Downloading argon2_cffi_bindings-21.2.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (86 kB)
Downloading cffi-1.17.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (479 kB)
Downloading beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)
Downloading soupsieve-2.7-py3-none-any.whl (36 kB)
Downloading decorator-5.2.1-py3-none-any.whl (9.2 kB)
Downloading defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)
Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)
Downloading ipython_pygments_lexers-1.1.1-py3-none-any.whl (8.1 kB)
Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)
Downloading arrow-1.3.0-py3-none-any.whl (66 kB)
Downloading types_python_dateutil-2.9.0.20250516-py3-none-any.whl (14 kB)
Downloading jupyterlab_pygments-0.3.0-py3-none-any.whl (15 kB)
Downloading nest_asyncio-1.6.0-py3-none-any.whl (5.2 kB)
Downloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (277 kB)
Downloading pycparser-2.22-py3-none-any.whl (117 kB)
Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)
Downloading stack_data-0.6.3-py3-none-any.whl (24 kB)
Downloading asttokens-3.0.0-py3-none-any.whl (26 kB)
Downloading executing-2.2.0-py2.py3-none-any.whl (26 kB)
Downloading pure_eval-0.2.3-py3-none-any.whl (11 kB)
Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)
Downloading wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)
Installing collected packages: webencodings, wcwidth, pure-eval, ptyprocess, fastjsonschema, websocket-client, webcolors, urllib3, uri-template, typing_extensions, types-python-dateutil, traitlets, tornado, tinycss2, soupsieve, sniffio, six, setuptools, send2trash, rpds-py, rfc3986-validator, pyzmq, pyyaml, python-json-logger, pygments, pycparser, psutil, prompt_toolkit, prometheus-client, platformdirs, pexpect, parso, pandocfilters, packaging, overrides, nest-asyncio, mistune, MarkupSafe, jupyterlab-pygments, jsonpointer, json5, idna, h11, fqdn, executing, defusedxml, decorator, debugpy, charset_normalizer, certifi, bleach, babel, attrs, async-lru, asttokens, terminado, stack_data, rfc3339-validator, requests, referencing, python-dateutil, matplotlib-inline, jupyter-core, jinja2, jedi, ipython-pygments-lexers, httpcore, comm, cffi, beautifulsoup4, anyio, jupyter-server-terminals, jupyter-client, jsonschema-specifications, ipython, httpx, arrow, argon2-cffi-bindings, jsonschema, isoduration, ipykernel, argon2-cffi, nbformat, nbclient, jupyter-events, nbconvert, jupyter-server, notebook-shim, jupyterlab-server, jupyter-lsp, jupyterlab
Successfully installed MarkupSafe-3.0.2 anyio-4.9.0 argon2-cffi-25.1.0 argon2-cffi-bindings-21.2.0 arrow-1.3.0 asttokens-3.0.0 async-lru-2.0.5 attrs-25.3.0 babel-2.17.0 beautifulsoup4-4.13.4 bleach-6.2.0 certifi-2025.6.15 cffi-1.17.1 charset_normalizer-3.4.2 comm-0.2.2 debugpy-1.8.14 decorator-5.2.1 defusedxml-0.7.1 executing-2.2.0 fastjsonschema-2.21.1 fqdn-1.5.1 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.10 ipykernel-6.29.5 ipython-9.3.0 ipython-pygments-lexers-1.1.1 isoduration-20.11.0 jedi-0.19.2 jinja2-3.1.6 json5-0.12.0 jsonpointer-3.0.0 jsonschema-4.24.0 jsonschema-specifications-2025.4.1 jupyter-client-8.6.3 jupyter-core-5.8.1 jupyter-events-0.12.0 jupyter-lsp-2.2.5 jupyter-server-2.16.0 jupyter-server-terminals-0.5.3 jupyterlab-4.4.3 jupyterlab-pygments-0.3.0 jupyterlab-server-2.27.3 matplotlib-inline-0.1.7 mistune-3.1.3 nbclient-0.10.2 nbconvert-7.16.6 nbformat-5.10.4 nest-asyncio-1.6.0 notebook-shim-0.2.4 overrides-7.7.0 packaging-25.0 pandocfilters-1.5.1 parso-0.8.4 pexpect-4.9.0 platformdirs-4.3.8 prometheus-client-0.22.1 prompt_toolkit-3.0.51 psutil-7.0.0 ptyprocess-0.7.0 pure-eval-0.2.3 pycparser-2.22 pygments-2.19.2 python-dateutil-2.9.0.post0 python-json-logger-3.3.0 pyyaml-6.0.2 pyzmq-27.0.0 referencing-0.36.2 requests-2.32.4 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 rpds-py-0.25.1 send2trash-1.8.3 setuptools-80.9.0 six-1.17.0 sniffio-1.3.1 soupsieve-2.7 stack_data-0.6.3 terminado-0.18.1 tinycss2-1.4.0 tornado-6.5.1 traitlets-5.14.3 types-python-dateutil-2.9.0.20250516 typing_extensions-4.14.0 uri-template-1.3.0 urllib3-2.5.0 wcwidth-0.2.13 webcolors-24.11.1 webencodings-0.5.1 websocket-client-1.8.0
(myenv) vboxuser@controlplane:~$ export SPARK_HOME=~/spark-4.0.0-bin-hadoop3
export PATH=$SPARK_HOME/bin:$PATH
export PYSPARK_PYTHON=python3
export PYSPARK_DRIVER_PYTHON=jupyter
export PYSPARK_DRIVER_PYTHON_OPTS="lab --ip=0.0.0.0 --port=8888 --no-browser"
(myenv) vboxuser@controlplane:~$ ls
custom-resources.yaml  helm-pysparkjupyter  kubeadm.config  kubeadmn.config  myenv  pv-pyspark  spark-4.0.0-bin-hadoop3  spark-RBAC.yaml  spark.yaml
(myenv) vboxuser@controlplane:~$ cd ./spark-4*/
(myenv) vboxuser@controlplane:~/spark-4.0.0-bin-hadoop3$ cd ./bin/
(myenv) vboxuser@controlplane:~/spark-4.0.0-bin-hadoop3/bin$ pyspark \
  --master k8s://https://192.168.1.150:6443 \
  --conf spark.submit.deployMode=client \
  --conf spark.driver.host=192.168.1.150 \
  --conf spark.driver.bindAddress=0.0.0.0 \
  --conf spark.executor.instances=2 \
  --conf spark.kubernetes.container.image=bitnami/spark:4.0.0 \
  --conf spark.kubernetes.executor.deleteOnTermination=true
usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]
               [--paths] [--json] [--debug]
               [subcommand]

Jupyter: Interactive Computing

positional arguments:
  subcommand     the subcommand to launch

options:
  -h, --help     show this help message and exit
  --version      show the versions of core jupyter packages and exit
  --config-dir   show Jupyter config dir
  --data-dir     show Jupyter data dir
  --runtime-dir  show Jupyter runtime dir
  --paths        show all Jupyter paths. Add --json for machine-readable
                 format.
  --json         output paths as machine-readable json
  --debug        output debug information about paths

Available subcommands: dejavu events execute kernel kernelspec lab
labextension labhub migrate nbconvert run server troubleshoot trust

Jupyter command `jupyter-from pyspark.util import spark_connect_mode; print(spark_connect_mode())` not found.
The environment variable SPARK_CONNECT_MODE has unknown value or pyspark.util package is not available:
[I 2025-06-26 11:37:05.455 ServerApp] jupyter_lsp | extension was successfully linked.
[I 2025-06-26 11:37:05.465 ServerApp] jupyter_server_terminals | extension was successfully linked.
[I 2025-06-26 11:37:05.475 ServerApp] jupyterlab | extension was successfully linked.
[I 2025-06-26 11:37:05.477 ServerApp] Writing Jupyter server cookie secret to /home/vboxuser/.local/share/jupyter/runtime/jupyter_cookie_secret
[I 2025-06-26 11:37:06.184 ServerApp] notebook_shim | extension was successfully linked.
[I 2025-06-26 11:37:06.232 ServerApp] notebook_shim | extension was successfully loaded.
[I 2025-06-26 11:37:06.237 ServerApp] jupyter_lsp | extension was successfully loaded.
[I 2025-06-26 11:37:06.240 ServerApp] jupyter_server_terminals | extension was successfully loaded.
[I 2025-06-26 11:37:06.245 LabApp] JupyterLab extension loaded from /home/vboxuser/myenv/lib/python3.12/site-packages/jupyterlab
[I 2025-06-26 11:37:06.246 LabApp] JupyterLab application directory is /home/vboxuser/myenv/share/jupyter/lab
[I 2025-06-26 11:37:06.248 LabApp] Extension Manager is 'pypi'.
[I 2025-06-26 11:37:06.383 ServerApp] jupyterlab | extension was successfully loaded.
[I 2025-06-26 11:37:06.386 ServerApp] Serving notebooks from local directory: /home/vboxuser/spark-4.0.0-bin-hadoop3/bin
[I 2025-06-26 11:37:06.388 ServerApp] Jupyter Server 2.16.0 is running at:
[I 2025-06-26 11:37:06.388 ServerApp] http://controlplane:8888/lab?token=9758af800dc2ea015e74706455cf2b904975f49b131a3a47
[I 2025-06-26 11:37:06.388 ServerApp]     http://127.0.0.1:8888/lab?token=9758af800dc2ea015e74706455cf2b904975f49b131a3a47
[I 2025-06-26 11:37:06.388 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 2025-06-26 11:37:06.397 ServerApp]

    To access the server, open this file in a browser:
        file:///home/vboxuser/.local/share/jupyter/runtime/jpserver-10885-open.html
    Or copy and paste one of these URLs:
        http://controlplane:8888/lab?token=9758af800dc2ea015e74706455cf2b904975f49b131a3a47
        http://127.0.0.1:8888/lab?token=9758af800dc2ea015e74706455cf2b904975f49b131a3a47
[I 2025-06-26 11:37:06.455 ServerApp] Skipped non-installed server(s): bash-language-server, dockerfile-language-server-nodejs, javascript-typescript-langserver, jedi-language-server, julia-language-server, pyright, python-language-server, python-lsp-server, r-languageserver, sql-language-server, texlab, typescript-language-server, unified-language-server, vscode-css-languageserver-bin, vscode-html-languageserver-bin, vscode-json-languageserver-bin, yaml-language-server
[W 2025-06-26 11:37:54.734 LabApp] Could not determine jupyterlab build status without nodejs
[I 2025-06-26 11:38:01.882 ServerApp] Creating new notebook in
[I 2025-06-26 11:38:02.052 ServerApp] Writing notebook-signing key to /home/vboxuser/.local/share/jupyter/notebook_secret
[I 2025-06-26 11:38:02.916 ServerApp] Kernel started: ca874685-72d4-41af-b116-02d43a4768e3
WARNING: Using incubator modules: jdk.incubator.vector
25/06/26 11:38:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[I 2025-06-26 11:38:52.974 ServerApp] Connecting to kernel ca874685-72d4-41af-b116-02d43a4768e3.
[W 2025-06-26 11:38:52.979 ServerApp] The websocket_ping_timeout (90000) cannot be longer than the websocket_ping_interval (30000).
    Setting websocket_ping_timeout=30000
[I 2025-06-26 11:38:53.003 ServerApp] Connecting to kernel ca874685-72d4-41af-b116-02d43a4768e3.
[I 2025-06-26 11:38:53.041 ServerApp] Connecting to kernel ca874685-72d4-41af-b116-02d43a4768e3.
[W 2025-06-26 11:38:53.255 ServerApp] Got events for closed stream <zmq.eventloop.zmqstream.ZMQStream object at 0x7c09c13d0ce0>
[I 2025-06-26 11:40:02.289 ServerApp] Saving file at /Untitled.ipynb
[I 2025-06-26 11:42:02.473 ServerApp] Saving file at /Untitled.ipynb
        [I 2025-06-26 11:44:03.668 ServerApp] Saving file at /Untitled.ipynb
[I 2025-06-26 11:48:05.820 ServerApp] Saving file at /Untitled.ipynb
[I 2025-06-26 11:54:08.200 ServerApp] Starting buffering for ca874685-72d4-41af-b116-02d43a4768e3:8abb621e-bcea-4545-a679-0422b263809f
[I 2025-06-26 11:54:18.264 ServerApp] New terminal with automatic name: 1
[I 2025-06-26 11:54:34.100 ServerApp] Kernel shutdown: ca874685-72d4-41af-b116-02d43a4768e3
[W 2025-06-26 11:54:35.517 ServerApp] delete /Untitled.ipynb


SINCE WE ARE USING THE VM AS THE CLIENT ON THE LOCAL NETWORK, WE ONLY NEED TO PUT ITS IP ON THE LOCAL NETWORK PLUS :8888 WE WILL ACCESS JUPYTER LAB WITH EVERYTHING CONFIGURED AND READY TO USE PYSPARK ON AN INTERACTIVE SESSION. 
IF WE WANT TO PERSIST PYSPARK CONFIGURATION WE CAN USE THE FOLLOWING COMMANDS:

(myenv) vboxuser@controlplane:~/spark-4.0.0-bin-hadoop3/bin$ nano ~/.bashrc


AT THE END OF THE OPENED FILE WE COPY THE FOLLOWING COMMANDS:

source ~/myenv/bin/activate

export SPARK_HOME=~/spark-4.0.0-bin-hadoop3
export PATH=$SPARK_HOME/bin:$PATH
export PYSPARK_PYTHON=python3
export PYSPARK_DRIVER_PYTHON=jupyter
export PYSPARK_DRIVER_PYTHON_OPTS="lab --ip=0.0.0.0 --port=8888 --no-browser"

AFTER WE HAVE FILLED THE FILE AND RETURNED TO THE CONSOLE WE USE THE FOLLOWING COMMAND TO PERSIST THE CHANGES IN THE BASH ENVIRONMENT VARIABLES

(myenv) vboxuser@controlplane:~/spark-4.0.0-bin-hadoop3/bin$ source ~/.bashrc

THEN WE CAN RUN

pyspark \
  --master k8s://https://192.168.1.150:6443 \
  --conf spark.submit.deployMode=client \
  --conf spark.driver.host=192.168.1.150 \
  --conf spark.driver.bindAddress=0.0.0.0 \
  --conf spark.executor.instances=2 \
  --conf spark.kubernetes.container.image=bitnami/spark:4.0.0

FINALLY IF WE WANT THE DRIVER TO BE INSIDE THE CLUSTER WE NEED TO BUILD A DOCKER IMAGE USING THE A DOCKERFILE. I HAVE INSTRUCTIONS TO BUILD IT IN "Spark docker image specifics and how to upload it.txt".

THANKS TO THE CONFIGURATION OF THE IMAGE WE CAN RUN THE SAME PYSPARK COMMAND INSIDE THE POD WITH MINIMAL CHANGES:

pyspark \
  --master k8s://https://192.168.1.150:6443 \
  --conf spark.submit.deployMode=client \
  --conf spark.driver.host=spark-driver-headless.default.svc.cluster.local \
  --conf spark.driver.bindAddress=0.0.0.0 \
  --conf spark.executor.instances=2 \
  --conf spark.kubernetes.container.image=bitnami/spark:3.5.6 \ # THAT WILL DEPEND ON THE VERSION OF SPARK WE USE
  --conf spark.kubernetes.executor.deleteOnTermination=true \
# HERE YOU PUT ALL THE PACKAGES YOU WANT, THESE TWO ARE FOR KAFKA AND ICEBERG RESPECTIVELY:
  --packages org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.0, org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.9.2
# YOU WANT TO CONNECT TO MINIO PUT:
  --conf spark.sql.catalog.my_catalog=org.apache.iceberg.spark.SparkCatalog \
  --conf spark.sql.catalog.my_catalog.type=hadoop \
  --conf spark.sql.catalog.my_catalog.warehouse=s3a://my-bucket/warehouse \
  --conf spark.hadoop.fs.s3a.access.key=DntpVaf6QiCkEvTXS5UH \
  --conf spark.hadoop.fs.s3a.secret.key=4IFrgFNlfvS73WGq0D0OQYdEhCoKw4tuzb7Msoaz \
  --conf spark.hadoop.fs.s3a.endpoint=http://myminio-hl.minio-tenant.svc.cluster.local:9000 \
  --conf spark.hadoop.fs.s3a.path.style.access=true

A MORE REGULAR METHOD IS TO ENTER INSIDE THE JUPYTER NOTEBOOK AND TYPE THE FOLLWING PYTHON COMMAND. TAKE INTO ACCOUNT THAT FIRST YOU NEED TO HAVE THE CLIENT POD RUNNING AND IN MY CASE I HAVE USED A NODEPORT TO ENTER THE JUPYTER SESSION THROUGH MY LAPTOP BROWSER THAT LOOKS AT PORT 8888 ON THE POD. 

BESIDES, YOU NEED TO CREATE A HEADLESS SERVICE THAT POINTS TO THE DRIVER POD, THAT WAY IT CAN BE EASILY REFERENCED USING THE DNS ADDRESS ON THE spark.driver.host COMMAND AS YOU SEE ON THE CODE

AND YOU ALSO HAVE TO INITIALIZE JUPYTERLAB USING THIS COMMAND ON THE BASH OF THE POD INSTEAD OF THE PYSPARK COMMAND AND THEN RUN INSIDE THE NOTEBOOK THE CODE:

jupyter lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root

NOW WE CAN INITIALIZE SPARK ON PYTHON EASILY:

from pyspark.sql import SparkSession

spark = (
    SparkSession.builder
    .appName("JupyterSparkApp")
    .master("k8s://https://192.168.1.150:6443")
    .config("spark.submit.deployMode", "client")
    .config("spark.driver.host", "spark-driver-headless.default.svc.cluster.local")
    .config("spark.driver.bindAddress", "0.0.0.0")
    .config("spark.executor.instances", "2")
    .config("spark.kubernetes.container.image", "bitnami/spark:3.5.6")
    .config("spark.kubernetes.executor.deleteOnTermination", "true")
    .config("spark.jars.packages", "org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.9.2,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.103.3")
    .config("spark.sql.catalog.my_catalog", "org.apache.iceberg.spark.SparkCatalog")
    .config("spark.sql.catalog.my_catalog.type", "hadoop")
    .config("spark.sql.catalog.my_catalog.warehouse", "s3a://test")
    .config("spark.hadoop.fs.s3a.access.key", "DntpVaf6QiCkEvTXS5UH")
    .config("spark.hadoop.fs.s3a.secret.key", "4IFrgFNlfvS73WGq0D0OQYdEhCoKw4tuzb7Msoaz")
    .config("spark.hadoop.fs.s3a.endpoint", "http://myminio-hl.minio-tenant.svc.cluster.local:9000")
    .config("spark.hadoop.fs.s3a.path.style.access", "true")
    .getOrCreate()
)
AND YOU ALSO HAVE TO INITIALIZE JUPYTERLAB USING THIS COMMAND ON THE BASH OF THE POD INSTEAD OF THE PYSPARK COMMAND AND THEN RUN INSIDE THE NOTEBOOK THE CODE ABOVE:

jupyter lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root


HERE ARE DIFFERENT INITIALIZING EXAMPLES DEPENDING ON HOW WHICH TENCHNOLOGIES WE WANT TO INTEGRATE WITH SPARK:

- USE OF ICEBERG INSIDE SPARK WITH A GENERIC HADOOP CATALOG:

from pyspark.sql import SparkSession
from pyspark.sql import Row

spark = (
    SparkSession.builder
    .appName("JupyterSparkApp")
    .master("k8s://https://192.168.1.150:6443")
    .config("spark.submit.deployMode", "client")
    .config("spark.driver.host", "spark-driver-headless.default.svc.cluster.local")
    .config("spark.driver.bindAddress", "0.0.0.0")
    .config("spark.executor.instances", "2")
    .config("spark.kubernetes.container.image", "bitnami/spark:3.5.6")
    .config("spark.kubernetes.executor.deleteOnTermination", "true")
    .config("spark.jars.packages", "org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.9.2,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262")
    .config("spark.sql.catalog.my_catalog", "org.apache.iceberg.spark.SparkCatalog")
    .config("spark.sql.catalog.my_catalog.type", "hadoop")
    .config("spark.sql.catalog.my_catalog.warehouse", "s3a://test")
    .config("spark.hadoop.fs.s3a.access.key", "DntpVaf6QiCkEvTXS5UH")
    .config("spark.hadoop.fs.s3a.secret.key", "4IFrgFNlfvS73WGq0D0OQYdEhCoKw4tuzb7Msoaz")
    .config("spark.hadoop.fs.s3a.endpoint", "http://myminio-hl.minio-tenant.svc.cluster.local:9000")
    .config("spark.hadoop.fs.s3a.path.style.access", "true")
    .getOrCreate()
)

data = [Row(id=1, nombre="Alice"), Row(id=2, nombre="Bob")]
df = spark.createDataFrame(data)
df.show()


spark.sql("""
CREATE TABLE IF NOT EXISTS my_catalog.demo_tabla (
    id INT,
    nombre STRING
)
USING iceberg
""")


df.writeTo("my_catalog.demo_tabla").append()

df_leido = spark.read.format("iceberg").table("my_catalog.demo_tabla")
df_leido.show()

- THIS ONE USES PROJECT NESSIE INSTEAD WHICH ENABLES VERSIONING OF THE CATALOG ON A GITHUB-LIKE BASIS. ¡¡¡TAKE INTO ACCOUNT THAT NESSIE MUST HAVE BEEN DEPLOYED FIRST!!! THE NESSIE CONFIGURATION THAT IS COMPATIBLE WITH THIS COMMAND IS ON THE NESSIE DIRECTORY HERE IN GITHUB.


from pyspark.sql import SparkSession
from pyspark.sql import Row

spark = (
    SparkSession.builder
    .appName("JupyterSparkApp")
    .master("k8s://https://192.168.1.150:6443")
    .config("spark.submit.deployMode", "client")
    .config("spark.driver.host", "spark-driver-headless.default.svc.cluster.local")
    .config("spark.driver.bindAddress", "0.0.0.0")
    .config("spark.executor.instances", "2")
    .config("spark.kubernetes.container.image", "bitnami/spark:3.5.6")
    .config("spark.kubernetes.executor.deleteOnTermination", "true")
    .config("spark.jars.packages", "org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.9.2,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.103.3,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262")
    .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions")
    .config("spark.sql.catalog.nessie", "org.apache.iceberg.spark.SparkCatalog")
    .config("spark.sql.catalog.nessie.uri", "http://nessie.minio-tenant.svc.cluster.local:19120/api/v1")
    .config("spark.sql.catalog.nessie.ref", "main")
    .config("spark.sql.catalog.nessie.authentication.type", "NONE")
    .config("spark.sql.catalog.nessie.catalog-impl", "org.apache.iceberg.nessie.NessieCatalog")
    .config("spark.sql.catalog.nessie.warehouse", "s3a://test")
    .config("spark.hadoop.fs.s3a.access.key", "DntpVaf6QiCkEvTXS5UH")
    .config("spark.hadoop.fs.s3a.secret.key", "4IFrgFNlfvS73WGq0D0OQYdEhCoKw4tuzb7Msoaz")
    .config("spark.hadoop.fs.s3a.endpoint", "http://myminio-hl.minio-tenant.svc.cluster.local:9000")
    .config("spark.hadoop.fs.s3a.path.style.access", "true")
    .getOrCreate()
)

data = [Row(id=1, nombre="Alice"), Row(id=2, nombre="Bob")]
df = spark.createDataFrame(data)
df.show()

spark.sql("""
CREATE TABLE IF NOT EXISTS nessie.nueva_tabla (
    id INT,
    nombre STRING
)
USING iceberg
""")

spark.sql("""
INSERT INTO nessie.nueva_tabla VALUES
(1, 'Alice'),
(2, 'Bob')
""")

spark.sql("SELECT * FROM nessie.nueva_tabla").show()

spark.sql("""
CREATE BRANCH dev IN nessie FROM main
""")

spark.sql("LIST REFERENCES IN nessie").show(truncate=False)

TO SEE ALL DEPLOYMENT DETAILS I HAVE POSTED THE  spark-driver.yaml YAML WITH ALL NECESSARY COMPONENTS TO BE USED IN kubectl apply -f spark-driver.yaml  









