{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05de030b-9495-4686-8a1a-00ff9f27cc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_json, struct, col, expr, row_number, from_json, get_json_object, explode, when\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StringType, IntegerType, MapType, StructField\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Creamos la sesión de Spark con configuración para Kubernetes\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"JupyterSparkApp\")\n",
    "    .master(\"k8s://https://192.168.1.150:6443\")\n",
    "    .config(\"spark.submit.deployMode\", \"client\")\n",
    "    .config(\"spark.driver.host\", \"spark-driver-headless.default.svc.cluster.local\")\n",
    "    .config(\"spark.driver.port\", \"7077\")\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "    .config(\"spark.executor.instances\", \"2\")\n",
    "    .config(\"spark.kubernetes.container.image\", \"docker.io/bitnami/spark:3.5.6\")\n",
    "    .config(\"spark.kubernetes.executor.deleteOnTermination\", \"true\")\n",
    "    .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.9.2,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.103.3,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\")\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\")\n",
    "    .config(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.nessie.uri\", \"http://nessie.nessie-ns.svc.cluster.local:19120/api/v1\")\n",
    "    .config(\"spark.sql.catalog.nessie.ref\", \"main\")\n",
    "    .config(\"spark.sql.catalog.nessie.authentication.type\", \"NONE\")\n",
    "    .config(\"spark.sql.catalog.nessie.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\")\n",
    "    .config(\"spark.sql.catalog.nessie.warehouse\", \"s3a://synthetic\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"qVgFWBabQmQrSuWTJGhj\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"l2GjPEVu22SfiqtaAU2zj3lBptEIoG1iRXGucn3o\")\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://myminio-hl.minio-tenant.svc.cluster.local:9000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ed51c4-887b-46a0-a9cc-14c6b9d346b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS nessie.orders_bronze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014384f5-89e7-4e9c-81f2-9c954caa9f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bronze layer\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"my-cluster-kafka-bootstrap.kafka.svc.cluster.local:9092\") \\\n",
    "    .option(\"subscribe\", \"mongo.synthetic.orders\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "df_iceberg = df.select(\n",
    "    col(\"key\").cast(\"string\"),\n",
    "    col(\"value\").cast(\"string\"),\n",
    "    \"topic\",\n",
    "    \"partition\",\n",
    "    \"offset\",\n",
    "    \"timestamp\",\n",
    "    \"timestampType\"\n",
    ")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS nessie.orders_bronze (\n",
    "    key STRING,\n",
    "    value STRING,\n",
    "    topic STRING,\n",
    "    partition INT,\n",
    "    offset LONG,\n",
    "    timestamp TIMESTAMP,\n",
    "    timestampType INT\n",
    ")\n",
    "USING iceberg\n",
    "PARTITIONED BY (days(timestamp))\n",
    "LOCATION 's3a://synthetic/orders_bronze'\n",
    "TBLPROPERTIES (\n",
    "    'format-version'='2',\n",
    "    'write.format.default'='parquet'\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "query = df_iceberg.writeStream \\\n",
    "    .format(\"iceberg\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"s3a://synthetic/checkpoints/orders_bronze\") \\\n",
    "    .toTable(\"nessie.orders_bronze\")\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57290c0-51d6-489a-8f2f-4eb72cf6d8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = spark.sql(\"\"\"\n",
    "       SELECT * FROM nessie.orders_bronze\n",
    "    \"\"\")\n",
    "query.show(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
