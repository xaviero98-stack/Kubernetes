{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfc5ab8d-1d22-4c64-8dc6-2999c772cc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-3f23eb95-b14b-4188-8942-230bba0cbbf1;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.9.2 in central\n",
      "\tfound org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.103.3 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 4271ms :: artifacts dl 179ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.9.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.103.3 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   16  |   0   |   0   |   0   ||   16  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-3f23eb95-b14b-4188-8942-230bba0cbbf1\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 16 already retrieved (0kB/55ms)\n",
      "25/10/19 09:26:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/19 09:26:05 WARN Utils: Service 'sparkDriver' could not bind on port 7077. Attempting port 7078.\n",
      "25/10/19 09:26:08 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_json, struct, col, expr, row_number, from_json, get_json_object, explode, when\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StringType, IntegerType, MapType, StructField\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Creamos la sesión de Spark con configuración para Kubernetes\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"JupyterSparkApp\")\n",
    "    .master(\"k8s://https://192.168.1.150:6443\")\n",
    "    .config(\"spark.submit.deployMode\", \"client\")\n",
    "    .config(\"spark.driver.host\", \"spark-driver-headless.default.svc.cluster.local\")\n",
    "    .config(\"spark.driver.port\", \"7077\")\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "    .config(\"spark.executor.instances\", \"2\")\n",
    "    .config(\"spark.kubernetes.container.image\", \"docker.io/bitnami/spark:3.5.6\")\n",
    "    .config(\"spark.kubernetes.executor.deleteOnTermination\", \"true\")\n",
    "    .config(\"spark.kubernetes.executor.nodeSelector\", \"node-role.kubernetes.io/control-plane=\")\n",
    "    .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.9.2,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.103.3,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\")\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\")\n",
    "    .config(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.nessie.uri\", \"http://nessie.nessie-ns.svc.cluster.local:19120/api/v1\")\n",
    "    .config(\"spark.sql.catalog.nessie.ref\", \"main\")\n",
    "    .config(\"spark.sql.catalog.nessie.authentication.type\", \"NONE\")\n",
    "    .config(\"spark.sql.catalog.nessie.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\")\n",
    "    .config(\"spark.sql.catalog.nessie.warehouse\", \"s3a://synthetic\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"qVgFWBabQmQrSuWTJGhj\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"l2GjPEVu22SfiqtaAU2zj3lBptEIoG1iRXGucn3o\")\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://myminio-hl.minio-tenant.svc.cluster.local:9000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd20710-85a0-49ab-8e18-4c94047e1730",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS nessie.products_silver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d908c20-c010-4513-9941-0cda2d8c9890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/19 09:27:21 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+----------+----+-------------+---+\n",
      "|oid                     |product_id|op  |ts_ms        |rn |\n",
      "+------------------------+----------+----+-------------+---+\n",
      "|68f3ea5916383378e218562d|NULL      |d   |1760865786197|1  |\n",
      "|68f3ea5916383378e218562d|PROD07    |r   |1760862255932|2  |\n",
      "|68f3ea5916383378e218562d|NULL      |NULL|NULL         |3  |\n",
      "|68f3ea5916383378e218562e|PROD08    |r   |1760862255940|1  |\n",
      "|68f3ea5916383378e218562f|PROD09    |r   |1760862255948|1  |\n",
      "|68f3ea5916383378e2185630|NULL      |d   |1760863766228|1  |\n",
      "|68f3ea5916383378e2185630|PROD10    |r   |1760862255941|2  |\n",
      "|68f3ea5916383378e2185630|NULL      |NULL|NULL         |3  |\n",
      "|68f3ea5916383378e2185631|PROD11    |r   |1760862255947|1  |\n",
      "|68f3ea5916383378e2185632|PROD12    |r   |1760862255941|1  |\n",
      "|68f3ea5916383378e2185633|NULL      |d   |1760865798371|1  |\n",
      "|68f3ea5916383378e2185633|PROD13    |r   |1760862255951|2  |\n",
      "|68f3ea5916383378e2185633|NULL      |NULL|NULL         |3  |\n",
      "|68f3ea5916383378e2185634|PROD14    |r   |1760862255948|1  |\n",
      "|68f3ea5916383378e2185635|PROD15    |r   |1760862255942|1  |\n",
      "|68f3ea5916383378e2185636|PROD16    |r   |1760862255945|1  |\n",
      "|68f3ea5916383378e2185637|PROD17    |r   |1760862255949|1  |\n",
      "|68f3ea5916383378e2185638|PROD18    |r   |1760862255952|1  |\n",
      "|68f3ea5916383378e2185639|PROD19    |r   |1760862255953|1  |\n",
      "|68f3ea5916383378e218563a|NULL      |d   |1760865041810|1  |\n",
      "|68f3ea5916383378e218563a|PROD20    |r   |1760862255943|2  |\n",
      "|68f3ea5916383378e218563a|NULL      |NULL|NULL         |3  |\n",
      "|68f3ea5916383378e218563b|PROD21    |r   |1760862255953|1  |\n",
      "|68f3ea5916383378e218563c|PROD22    |r   |1760862255944|1  |\n",
      "|68f3ea5916383378e218563d|PROD23    |r   |1760862255954|1  |\n",
      "|68f3ea5916383378e218563e|PROD24    |r   |1760862255950|1  |\n",
      "|68f3ea5916383378e218563f|PROD25    |r   |1760862255946|1  |\n",
      "|68f3ea5916383378e2185640|PROD26    |r   |1760862255946|1  |\n",
      "|68f3ea5916383378e2185641|PROD27    |r   |1760862255955|1  |\n",
      "|68f3ea5916383378e2185642|PROD28    |r   |1760862255947|1  |\n",
      "+------------------------+----------+----+-------------+---+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bronze = spark.read.format(\"iceberg\").table(\"nessie.products_bronze\")\n",
    "\n",
    "payload_schema = StructType().add(\"payload\", MapType(StringType(), StringType()))\n",
    "oid_schema = StructType([StructField(\"$oid\", StringType())])\n",
    "\n",
    "\n",
    "df_parsed = df_bronze.selectExpr(\"CAST(value AS STRING) as json_str\", \"CAST(key AS STRING) as json_key\") \\\n",
    "    .withColumn(\"value\", from_json(col(\"json_str\"), payload_schema)) \\\n",
    "    .withColumn(\"key\", from_json(col(\"json_key\"), payload_schema)) \\\n",
    "    .withColumn(\"key_id\", from_json(col(\"key.payload\")[\"id\"], oid_schema)) \\\n",
    "    .select(\"value\", \"key_id\")\n",
    "\n",
    "df_parsed_cols = df_parsed.select(\n",
    "    col(\"key_id.$oid\").alias(\"oid\"),\n",
    "    col(\"value.payload\")[\"before\"].alias(\"before\"),\n",
    "    col(\"value.payload\")[\"after\"].alias(\"after\"),\n",
    "    col(\"value.payload\")[\"source\"].alias(\"source\"),\n",
    "    col(\"value.payload\")[\"op\"].alias(\"op\"),\n",
    "    col(\"value.payload\")[\"ts_ms\"].alias(\"ts_ms\"),\n",
    "    col(\"value.payload\")[\"transaction\"].alias(\"transaction\"))\n",
    "\n",
    "product_type_schema = StructType([\n",
    "    StructField(\"type_id\", StringType()),\n",
    "    StructField(\"type_name\", StringType()),\n",
    "    StructField(\"type_description\", StringType())\n",
    "])\n",
    "\n",
    "product_schema = StructType([\n",
    "    StructField(\"_id\", StructType([StructField(\"$oid\", StringType())])),\n",
    "    StructField(\"product_id\", StringType()),\n",
    "    StructField(\"product_name\", StringType()),\n",
    "    StructField(\"product_price\", StringType()),\n",
    "    StructField(\"product_description\", StringType()),\n",
    "    StructField(\"product_type\", product_type_schema)\n",
    "])\n",
    "    \n",
    "df_if = df_parsed_cols.withColumn(\"data_map\", from_json(col(\"after\"), product_schema))\n",
    "\n",
    "df_BIGINT = df_if.withColumn(\"ts_ms\", col(\"ts_ms\").cast(\"bigint\"))\n",
    "\n",
    "df_final = df_BIGINT.select(\n",
    "        col(\"oid\"),\n",
    "        col(\"data_map\")[\"product_id\"].alias(\"product_id\"),\n",
    "        col(\"data_map\")[\"product_name\"].alias(\"product_name\"),\n",
    "        col(\"data_map\")[\"product_price\"].alias(\"product_price\"),\n",
    "        col(\"data_map\")[\"product_description\"].alias(\"product_description\"),\n",
    "        col(\"data_map\")[\"product_type\"][\"type_id\"].alias(\"type_id\"),\n",
    "        col(\"data_map\")[\"product_type\"][\"type_name\"].alias(\"type_name\"),\n",
    "        col(\"data_map\")[\"product_type\"][\"type_description\"].alias(\"type_description\"),\n",
    "        col(\"op\").alias(\"op\"),\n",
    "        col(\"ts_ms\")\n",
    "    )\n",
    "\n",
    "window = Window.partitionBy(\"oid\").orderBy(col(\"ts_ms\").desc())\n",
    "\n",
    "df_final_batch = df_final.withColumn(\"rn\", row_number().over(window)) \\\n",
    "                         .select(\"oid\", \"product_id\", \"op\", \"ts_ms\", \"rn\")\n",
    "\n",
    "df_final_batch.show(30, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8580da-d36c-4ade-aef7-70410a175350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silver layer\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS nessie.products_silver (\n",
    "  oid STRING,\n",
    "  product_id STRING,\n",
    "  product_name STRING,\n",
    "  product_price STRING,\n",
    "  product_description STRING,\n",
    "  type_id STRING,\n",
    "  type_name STRING,\n",
    "  type_description STRING,\n",
    "  op STRING,\n",
    "  ts_ms BIGINT\n",
    ")\n",
    "USING iceberg\n",
    "PARTITIONED BY (bucket(16, product_id)) -- 16 buckets\n",
    "LOCATION 's3a://synthetic/products-silver'\n",
    "\"\"\")\n",
    "\n",
    "df_bronze = spark.readStream.format(\"iceberg\").table(\"nessie.products_bronze\")\n",
    "\n",
    "payload_schema = StructType().add(\"payload\", MapType(StringType(), StringType()))\n",
    "oid_schema = StructType([StructField(\"$oid\", StringType())])\n",
    "\n",
    "\n",
    "df_parsed = df_bronze.selectExpr(\"CAST(value AS STRING) as json_str\", \"CAST(key AS STRING) as json_key\") \\\n",
    "    .withColumn(\"value\", from_json(col(\"json_str\"), payload_schema)) \\\n",
    "    .withColumn(\"key\", from_json(col(\"json_key\"), payload_schema)) \\\n",
    "    .withColumn(\"key_id\", from_json(col(\"key.payload\")[\"id\"], oid_schema)) \\\n",
    "    .select(\"value\", \"key_id\")\n",
    "\n",
    "df_parsed_cols = df_parsed.select(\n",
    "    col(\"key_id.$oid\").alias(\"oid\"),\n",
    "    col(\"value.payload\")[\"before\"].alias(\"before\"),\n",
    "    col(\"value.payload\")[\"after\"].alias(\"after\"),\n",
    "    col(\"value.payload\")[\"source\"].alias(\"source\"),\n",
    "    col(\"value.payload\")[\"op\"].alias(\"op\"),\n",
    "    col(\"value.payload\")[\"ts_ms\"].alias(\"ts_ms\"),\n",
    "    col(\"value.payload\")[\"transaction\"].alias(\"transaction\"))\n",
    "\n",
    "product_type_schema = StructType([\n",
    "    StructField(\"type_id\", StringType()),\n",
    "    StructField(\"type_name\", StringType()),\n",
    "    StructField(\"type_description\", StringType())\n",
    "])\n",
    "\n",
    "product_schema = StructType([\n",
    "    StructField(\"_id\", StructType([StructField(\"$oid\", StringType())])),\n",
    "    StructField(\"product_id\", StringType()),\n",
    "    StructField(\"product_name\", StringType()),\n",
    "    StructField(\"product_price\", StringType()),\n",
    "    StructField(\"product_description\", StringType()),\n",
    "    StructField(\"product_type\", product_type_schema)\n",
    "])\n",
    "    \n",
    "df_if = df_parsed_cols.withColumn(\"data_map\", from_json(col(\"after\"), product_schema))\n",
    "\n",
    "df_BIGINT = df_if.withColumn(\"ts_ms\", col(\"ts_ms\").cast(\"bigint\"))\n",
    "\n",
    "df_final = df_BIGINT.select(\n",
    "        col(\"oid\"),\n",
    "        col(\"data_map\")[\"product_id\"].alias(\"product_id\"),\n",
    "        col(\"data_map\")[\"product_name\"].alias(\"product_name\"),\n",
    "        col(\"data_map\")[\"product_price\"].alias(\"product_price\"),\n",
    "        col(\"data_map\")[\"product_description\"].alias(\"product_description\"),\n",
    "        col(\"data_map\")[\"product_type\"][\"type_id\"].alias(\"type_id\"),\n",
    "        col(\"data_map\")[\"product_type\"][\"type_name\"].alias(\"type_name\"),\n",
    "        col(\"data_map\")[\"product_type\"][\"type_description\"].alias(\"type_description\"),\n",
    "        col(\"op\").alias(\"op\"),\n",
    "        col(\"ts_ms\")\n",
    "    )\n",
    "\n",
    "# YOU NEED A FUNCTION THAT LOOKS AT ts_ms AND TAKES THE MOST RECENT RSTATE OF EACH PRODUCT DEPENDING ON WHETHER IT IS AN UPDATE A DELETE OR AN INSERT. \n",
    "# WE WILL USE BTACH SYNTAX AND FUNCTIONALITIES FOR EACH STREAMING BATCH TO HEKP US LIKE WE CAN SEE ON THE FOLLOWING CODE\n",
    "def merge_into_products_silver(microBatchOutputDF, batchId):\n",
    "\n",
    "    window = Window.partitionBy(\"oid\").orderBy(col(\"ts_ms\").desc())\n",
    "\n",
    "    df_final_batch = microBatchOutputDF.withColumn(\"rn\", row_number().over(window)) \\\n",
    "                      .filter(col(\"rn\") == 1) \\\n",
    "                      .drop(\"rn\")\n",
    "    \n",
    "    df_final_batch.createOrReplaceTempView(\"batch_updates\")\n",
    "\n",
    "    microBatchOutputDF.sparkSession.sql(\"\"\"\n",
    "        MERGE INTO nessie.products_silver AS target\n",
    "        USING batch_updates AS source\n",
    "        ON target.oid = source.oid\n",
    "        WHEN MATCHED AND source.op = 'd' AND source.ts_ms > target.ts_ms THEN DELETE\n",
    "        WHEN MATCHED AND source.op IN ('u', 'r', 'i') AND source.ts_ms > target.ts_ms THEN UPDATE SET *\n",
    "        WHEN NOT MATCHED AND source.op IN ('c', 'r', 'i') THEN INSERT *\n",
    "    \"\"\")\n",
    "    \n",
    "#    microBatchOutputDF.sparkSession.sql(\"\"\"\n",
    "#        DELETE FROM nessie.products_silver AS target\n",
    "#        WHERE target.product_id IN (SELECT product_id FROM batch_updates) \n",
    "#        AND target.oid NOT IN (SELECT oid FROM batch_updates)\n",
    "#    \"\"\")\n",
    "\n",
    "query = df_final.writeStream \\\n",
    "    .foreachBatch(merge_into_products_silver) \\\n",
    "    .option(\"checkpointLocation\", \"s3a://synthetic/checkpoints/products_silver\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .start()\n",
    "query.awaitTermination()\n",
    "# USE /workspace/kafka-spark so you can delete checkpoints folder and rerun the process from the first topic offset if you run from earliest everytime\n",
    "# ON THE CLEAN VERSION YOU WILL USE s3 BUCKET TO SAVE THE CHECKPOINTS\n",
    "# REMEMBER THAT STOPPING THIS CELL WON'T MAKE THE SPARK STREAMS JOB STOP, IT WILL CONTINUE UNLESS YOU USE SPARK COMMANDS TO STOP IT. HERE IS NOT NECESSARY TO DO IT SO WE WILL NOT USE IT.\n",
    "# YOU HAVE TO DISCOVER WHY THE CONTROLPLANE CAN'T HOST SPARK EXECUTORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8331062a-fa04-4217-996b-4808583713d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<pyspark.sql.streaming.query.StreamingQuery at 0x7a63c44a5e50>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a615c092-f1ff-4d55-a122-55e6869f3edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ec51840-cdb8-4193-969f-7bad16d40bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+-------------+--------------------+-------+--------------------+--------------------+---+-------------+\n",
      "|                 oid|product_id|        product_name|product_price| product_description|type_id|           type_name|    type_description| op|        ts_ms|\n",
      "+--------------------+----------+--------------------+-------------+--------------------+-------+--------------------+--------------------+---+-------------+\n",
      "|68f3ea5916383378e...|    PROD11| Adjustable LED Lamp|        $9.37|Computer his toug...|   Furn|   Furniture / Decor|Alone must someti...|  r|1760862255947|\n",
      "|68f3ea5916383378e...|    PROD16|     Sports Sneakers|      $483.37|   Rise when manage.|   Spor|   Sports / Footwear|Others owner majo...|  r|1760862255945|\n",
      "|68f3ea5916383378e...|    PROD22|   Adjustable Tripod|      $369.39|Relationship rest...|   Phot|         Photography|Us long anyone av...|  r|1760862255944|\n",
      "|68f3ea5916383378e...|    PROD23|  Portable LED Flash|      $254.33|Style always alon...|   Phot|         Photography|On product other ...|  r|1760862255954|\n",
      "|68f3ea5916383378e...|    PROD24|Stainless Steel C...|   $72,142.31|Whatever take rec...|   Home|      Home / Kitchen|Do institution in...|  r|1760862255950|\n",
      "|68f3ea5916383378e...|    PROD25|  Classic Wristwatch|       $97.16|Within sell wheth...|   Fash|Fashion / Accesso...|Only happy rather...|  r|1760862255946|\n",
      "|68f3ea5916383378e...|    PROD26|Polarized Sunglasses|      $857.18|South other feel ...|   Fash|Fashion / Accesso...|Bar economy real ...|  r|1760862255946|\n",
      "|68f3ea5916383378e...|    PROD29|      Unisex Perfume|    $4,934.64|Whom economy add ...|   Beau|  Beauty / Cosmetics|Official somethin...|  r|1760862255956|\n",
      "|68f3ea5916383378e...|    PROD30|Facial Moisturize...|        $2.56|Herself finally u...|   Beau|  Beauty / Cosmetics|Together alone di...|  r|1760862255944|\n",
      "|68f3ea5916383378e...|    PROD08|Multi-function Bl...|   $43,844.14|Only century cold...|   Home|      Home / Kitchen|Little station re...|  r|1760862255940|\n",
      "|68f3ea5916383378e...|    PROD12|       3-Seater Sofa|      $404.65|Property cut medi...|   Furn|    Furniture / Home|Lot trade experie...|  r|1760862255941|\n",
      "|68f3ea5916383378e...|    PROD14|Official Soccer Ball|       $32.63|Support part pain...|   Spor|    Sports / Outdoor|Base simple no at...|  r|1760862255948|\n",
      "|68f3ea5916383378e...|    PROD15|     Hiking Backpack|    $5,060.22|Administration re...|   Spor|    Sports / Outdoor|History shoulder ...|  r|1760862255942|\n",
      "|68f3ea5916383378e...|    PROD19|         Gel Ink Pen|        $6.29|Sign region blood...|   Stat| Stationery / Office|Ask participant m...|  r|1760862255953|\n",
      "|68f3ea5916383378e...|    PROD27|   Waterproof Jacket|   $75,885.46|Feeling forward s...|   Fash|  Fashion / Clothing|Keep create wait ...|  r|1760862255955|\n",
      "|68f3ea5916383378e...|    PROD28|      Thermal Gloves|    $7,849.97|Near Mr wind argu...|   Fash|  Fashion / Clothing|Involve difficult...|  r|1760862255947|\n",
      "|68f3ea5916383378e...|    PROD18|Professional A4 N...|      $943.32|Sell police ask h...|   Stat| Stationery / Office|Yet car now parti...|  r|1760862255952|\n",
      "+--------------------+----------+--------------------+-------------+--------------------+-------+--------------------+--------------------+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = spark.sql(\"\"\"\n",
    "       SELECT * FROM nessie.products_silver\n",
    "       SORT BY oid\n",
    "    \"\"\")\n",
    "query.show(30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
